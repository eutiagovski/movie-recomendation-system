{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando um dataset de Filmes e Séries de TV a partir do IMDB\n",
    "\n",
    "O objetivo deste notebook é realizar a coleta e a compilação dos dados de filmes e séries de tv do site IMDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install libraries\n",
    "# !pip install pandas selenium tqdm ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import List, TypeVar\n",
    "\n",
    "T = TypeVar('T')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtendo a descrição dos filmes do site IMDB via WebScrapping\n",
    "\n",
    "Nesta primeira parte, estaremos coletando os resumos dos filmes e séries de TV diretamente do site IMDB utilizando a técnica de web scrapping, que é uma técnica de obtenção de dados via automação de código diretamente do navegador.\n",
    "\n",
    "O IMDB possui uma ampla base dados, com cerca de XXX filmes e séries de TV. \n",
    "\n",
    "Como o objetivo final deste trabalho é desenvolver um algoritmo de recomendação de filmes e/ou séries, optamos por aplicar filtros a fim de diminuir o tamanho geral do dataset e evitar problemas de lentidão e memória.\n",
    "\n",
    "Estaremos buscando dados de filmes e séries de TV que tenham avaliação geral maior que 7 e que tenham recebidos mais de 1 mil votos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start webriver\n",
    "driver = webdriver.Firefox() # start driver\n",
    "\n",
    "# go to imdb site and assert the language is english\n",
    "driver.get('https://www.imdb.com')\n",
    "\n",
    "assert driver.title == 'IMDb: Classificações, avaliações e onde assistir os melhores filmes e séries'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_english_language():\n",
    "    if not driver.find_element(By.XPATH, '/html/body/div[2]/nav/div[2]/div[6]/label/span').text == 'EN':\n",
    "        driver.find_element(By.XPATH, '/html/body/div[2]/nav/div[2]/div[6]').click() # click on dropdown language set\n",
    "        sleep(1)\n",
    "        driver.find_element(By.XPATH, '/html/body/div[2]/nav/div[2]/div[6]/div/div/div/span/ul[1]/li[3]/span[2]').click() # click on English language on the list\n",
    "        sleep(1)\n",
    "\n",
    "# select_english_language()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a base url to get the movies\n",
    "\n",
    "def create_base_url(min_ratings: str,\n",
    "                    max_ratings: str,\n",
    "                    min_votes: str = 1000,\n",
    "                    title_types: List[str] = ['feature', 'tv_series'],\n",
    "                    filter_type: str = 'release_date',\n",
    "                    order_type: str= 'desc') -> str:\n",
    "    # create a url based on the given filters\n",
    "    return f'https://www.imdb.com/search/title/?title_type={\",\".join(title_types)}&user_rating={min_ratings},{max_ratings}&sort={filter_type},{order_type}&num_votes={min_votes},999999999999'\n",
    "\n",
    "# navigate to imdb url\n",
    "# driver.get(create_base_url(min_ratings=5, max_ratings=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10415"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_total_titles():\n",
    "    total_size = int(driver.find_element(By.XPATH, '/html/body/div[2]/main/div[2]/div[3]/section/section/div/section/section/div[2]/div/section/div[2]/div[2]/div[1]/div[1]').text.split(' ')[-1].replace('.', ''))\n",
    "    return total_size\n",
    "\n",
    "get_total_titles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since the imdb shows only 50 titles by time, we have to scroll the page to load all titles\n",
    "def load_page_items(batch_size: int= 10000):\n",
    "    \"\"\"Scoll the page until reach the batch size given\"\"\"\n",
    "    # grab total titles with the given filter\n",
    "    total_size = int(driver.find_element(By.XPATH, '/html/body/div[2]/main/div[2]/div[3]/section/section/div/section/section/div[2]/div/section/div[2]/div[2]/div[1]/div[1]').text.split(' ')[-1].replace('.', ''))\n",
    "\n",
    "    # check how many titles are loaded on screen\n",
    "    size = len(driver.find_elements(By.XPATH, '/html/body/div[2]/main/div[2]/div[3]/section/section/div/section/section/div[2]/div/section/div[2]/div[2]/ul/li'))\n",
    "\n",
    "    with tqdm(range(batch_size), desc='Carregando títulos', leave=False) as pbar:\n",
    "        # while the size of titles in screen is less then the total, we click on \"load more\" button to load the other titles\n",
    "        while not size >= batch_size:\n",
    "            try:\n",
    "                try:\n",
    "                    driver.find_element(By.CSS_SELECTOR, 'button.ipc-btn--single-padding:nth-child(1)').click()\n",
    "                    size = len(driver.find_elements(By.XPATH, '/html/body/div[2]/main/div[2]/div[3]/section/section/div/section/section/div[2]/div/section/div[2]/div[2]/ul/li'))\n",
    "                    pbar.update(50)\n",
    "                except:\n",
    "                    pass\n",
    "                sleep(1)\n",
    "            except KeyboardInterrupt:\n",
    "                break\n",
    "\n",
    "# load_page_items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset with the titles loaded on screen\n",
    "def get_elements_from_screen(df: List[T] = pd.DataFrame()) -> List[T]:\n",
    "    \"\"\"Return all the elements loaded on screen in a dadaset\"\"\"\n",
    "    # store the elements in a list\n",
    "    elements = driver.find_elements(By.XPATH, '/html/body/div[2]/main/div[2]/div[3]/section/section/div/section/section/div[2]/div/section/div[2]/div[2]/ul/li')\n",
    "\n",
    "    with tqdm(range(len(elements)), desc=\"Criando dataset\", leave=False) as pbar:\n",
    "        # for each element we grab the infos we need and add to main df\n",
    "        for i, element in enumerate(elements):\n",
    "            tconst = element.find_element(By.XPATH, './div/div/div/div[1]/div[2]/div[1]/a').get_attribute('href').split('/')[4]     # tconst\n",
    "            primaryTitle = element.find_element(By.XPATH, './div/div/div/div[1]/div[2]/div[1]/a').text                              # primaryTitle\n",
    "            imdb_link = element.find_element(By.XPATH, './div/div/div/div[1]/div[2]/div[1]/a').get_attribute('href')                # imdb link\n",
    "            \n",
    "            try:\n",
    "                overview = element.find_element(By.XPATH, './div/div/div/div[2]/div[1]/div').text                                   # description\n",
    "            except:\n",
    "                overview = \"\"\n",
    "            \n",
    "            try:\n",
    "                image_url = element.find_element(By.XPATH, './div/div/div/div[1]/div[1]/div/div[2]/img').get_attribute('src')       # image_url\n",
    "            except:\n",
    "                image_url = \"\"\n",
    "\n",
    "            item = {\n",
    "                'tconst': [tconst],\n",
    "                'primaryTitle': [primaryTitle],\n",
    "                'overview': [overview],\n",
    "                'imdb_link': [imdb_link],\n",
    "                'image_url': [image_url]\n",
    "\n",
    "            }\n",
    "\n",
    "            df = pd.concat([df, pd.DataFrame(item)], axis=0)\n",
    "            df.drop_duplicates(subset=['tconst'], inplace=True)\n",
    "            df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "    return df\n",
    "\n",
    "# get_elements_from_screen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_backup(df: List[T], path: str) -> bool:\n",
    "    \"\"\"Make a backup for the given df and path\"\"\"\n",
    "    try:\n",
    "        df.to_csv(path, sep='\\t', index=False)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "def load_backup(path: str) -> List[T]:\n",
    "    \"\"\"Load a backup or create a empty dataset\"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(path, sep='\\t')\n",
    "    except:\n",
    "        return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size is 10415\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95353b6862bb4df19bde38ec7f200e61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Carregando títulos:   0%|          | 0/5209 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of elements loaded on screen is 5250\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "748b7a23ce6e4689beeb0b75a58de678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Criando dataset:   0%|          | 0/5250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size is 10415\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d46c637890584bb4806b40b65dc99ca1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Carregando títulos:   0%|          | 0/5209 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of elements loaded on screen is 5250\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70c3b3c5dec24df7a4392621d7ea3e7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Criando dataset:   0%|          | 0/5250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math \n",
    "\n",
    "overview_dataset_path = '../datasets/titles.overviews.pt-br.tsv'\n",
    "\n",
    "def run():\n",
    "    MIN_RATING = 6\n",
    "    MAX_RATING = 10\n",
    "\n",
    "    # carrega o backup\n",
    "    df = load_backup(overview_dataset_path)\n",
    "\n",
    "    # for each rating interval (by one) in the min max rating interval\n",
    "    for i in range(MIN_RATING, MAX_RATING):\n",
    "        # create a base url for that interval\n",
    "        base_url = create_base_url(i, i+1, order_type='asc')\n",
    "        driver.get(base_url)\n",
    "\n",
    "        # discover the total titles in the given parameters\n",
    "        total_items = get_total_titles()\n",
    "\n",
    "        # set the default batch size\n",
    "        batch_size = 10000\n",
    "        batch_range = 1\n",
    "\n",
    "        # if total size is larger then 10000, split the interval in two parts\n",
    "        if total_items >= 10000:\n",
    "            batch_size = math.ceil(total_items / 2)\n",
    "            batch_range = 2\n",
    "\n",
    "        # for each interval\n",
    "        for b in range(batch_range):\n",
    "            # modify the base url to match the interval\n",
    "            if b % 2 == 0:\n",
    "                base_url = create_base_url(i, i+1, order_type='desc')\n",
    "            else:\n",
    "                base_url = create_base_url(i, i+1, order_type='asc')\n",
    "\n",
    "            # assert the page is in the correct interval\n",
    "            driver.get(base_url)\n",
    "\n",
    "            # load all items for the given interval and batch size\n",
    "            load_page_items(batch_size+1)\n",
    "\n",
    "            # load elements in the screen\n",
    "            df = get_elements_from_screen(df)\n",
    "\n",
    "            # save a backup\n",
    "            make_backup(df, overview_dataset_path)\n",
    "            \n",
    "    return df\n",
    "df = run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to do - translate the titles whos not in pt-br\n",
    "\n",
    "!pip uninstall googletrans"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
